### **CausalDragonnetText Data Flow**

```mermaid
graph TD
    Input[("Input: List of Text Documents<br>(Batch Size)")] 
    --> Process[("Preprocessing & Chunking<br>(SentenceTransformer)")]
    
    subgraph "1. Text Embedding Phase"
    Process --> Embeds["Chunk Embeddings Tensor<br>(Batch Size, Max Chunks Per Document, Embedding Dimension)"]
    end

    subgraph "2. Feature Extractor (Confounder Discovery)"
    Embeds --> Norm["L2 Normalization<br>(Batch Size, Max Chunks Per Document, Embedding Dimension)"]
    Norm --> Filters["Confounder Filters (1D Convolution)<br>Filters: (Total Confounders * Features Per Confounder, Embedding Dimension, 1)"]
    Filters --> Scores["Confounder Match Scores<br>(Batch Size, Total Confounders * Features Per Confounder, Max Chunks Per Document)"]
    Scores --> Agg["Aggregator (e.g., Attention/Max Pooling)<br>Reduces 'Max Chunks' dimension"]
    Agg --> Pooled["Confounder Features<br>(Batch Size, Total Confounders * Features Per Confounder)"]
    end

    subgraph "3. DragonNet Representation"
    Pooled --> Rep1["FC Layers 1-5 (ReLU)<br>(Batch Size, Representation Dimension)"]
    Rep1 --> Rep6["FC Layer 6 (ELU)<br>(Batch Size, Representation Dimension)"]
    Rep6 --"This is 'phi'"--> Phi["Shared Representation (Phi)<br>(Batch Size, Representation Dimension)"]
    end

    subgraph "4. Causal Heads"
    Phi --> PropHead["Propensity Head (Linear)<br>(Batch Size, 1)"]
    PropHead --> T_Logit["Treatment Logit (t_logit)"]
    
    Phi --> Y0_Head["Outcome Y0 Head (ReLU -> ELU -> Linear)<br>(Batch Size, 1)"]
    Y0_Head --> Y0_Logit["Control Outcome Logit (y0_logit)"]

    Phi --> Y1_Head["Outcome Y1 Head (ReLU -> ELU -> Linear)<br>(Batch Size, 1)"]
    Y1_Head --> Y1_Logit["Treated Outcome Logit (y1_logit)"]
    end
```

### **Detailed Tensor Shape Transformation**

**1. Inputs & Embeddings**
*   **Input:** `raw_text` (List of strings)
*   **Tokenization/Encoding:** Splits text into chunks of e.g. 128 words.
*   **Output:** `chunk_embeddings`
    *   **Shape:** `(batch_size, max_chunks_per_document, embedding_dimension)`
    *   *Note: `embedding_dimension` is typically 384 or 768.*

**2. Feature Extractor**
*   **Input:** `chunk_embeddings`
*   **Operation:** `Conv1d` (Dot product with learnable confounder vectors)
    *   Acts as a "semantic search" scanning every chunk for every confounder.
*   **Intermediate:** `match_scores`
    *   **Shape:** `(batch_size, extractor_output_dimension, max_chunks_per_document)`
    *   *Where `extractor_output_dimension` = `num_confounders` Ã— `features_per_confounder`*
*   **Operation:** `Aggregator` (e.g., Attention)
    *   Collapses the "Chunks" dimension. Finds the "best" match for each confounder in the document.
*   **Output:** `confounder_features`
    *   **Shape:** `(batch_size, extractor_output_dimension)`

**3. DragonNet Representation**
*   **Input:** `confounder_features`
*   **Operation:** 6 Layers of Dense (Linear) transformations.
*   **Output:** `phi` (Shared Representation)
    *   **Shape:** `(batch_size, representation_dimension)`
    *   *Note: Typically 200 or 128.*

**4. Prediction Heads**
*   **Input:** `phi`
*   **Propensity Head:** 1 Linear Layer
    *   **Output:** `t_logit` `(batch_size, 1)`
*   **Outcome Heads (Y0 & Y1):** 3 Layers (Linear -> ReLU -> Linear -> ELU -> Linear)
    *   **Output:** `y0_logit` `(batch_size, 1)`
    *   **Output:** `y1_logit` `(batch_size, 1)`
